import torch
from attention.models.transformer import PositionalEncoding

# def test_positional_encoding():
#     d_hid = 512
#     n_position = 200
#     rand_word_vec = torch.rand(d_hid)
#     position_enc = PositionalEncoding(
#         d_hid = d_hid,
#         n_position = n_position
#     )
